{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HotpotQA 데이터셋을 로드합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data:   1%|          | 3.26M/566M [01:00<3:19:35, 47.0kB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# 데이터 로드 및 변환 함수 실행\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[43mload_and_convert_hotpotqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_qwen_file_hotpotqa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m스크립트 실행 중 예상치 못한 오류 발생: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36mload_and_convert_hotpotqa\u001b[1;34m(output_filepath)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 'hotpotqa' 데이터셋의 'fullwiki' 설정을 사용하고 'train' 스플릿을 로드\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# datasets 라이브러리가 자동으로 다운로드 및 로드합니다.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhotpot_qa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfullwiki\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터셋 로드 완료.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터셋 크기: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\load.py:2084\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m   2083\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2084\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2093\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2094\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2095\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\builder.py:925\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    924\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    926\u001b[0m     dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m    927\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    930\u001b[0m )\n\u001b[0;32m    931\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\builder.py:1649\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[1;32m-> 1649\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1650\u001b[0m         dl_manager,\n\u001b[0;32m   1651\u001b[0m         verification_mode,\n\u001b[0;32m   1652\u001b[0m         check_duplicate_keys\u001b[38;5;241m=\u001b[39mverification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS\n\u001b[0;32m   1653\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS,\n\u001b[0;32m   1654\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs,\n\u001b[0;32m   1655\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\builder.py:979\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    977\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[0;32m    978\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[1;32m--> 979\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_generators_kwargs)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[1;32mD:\\huggingface_cache\\modules\\datasets_modules\\datasets\\hotpot_qa\\133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5\\hotpot_qa.py:116\u001b[0m, in \u001b[0;36mHotpotQA._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfullwiki\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    114\u001b[0m     paths[datasets\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mTEST] \u001b[38;5;241m=\u001b[39m _URL_BASE \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhotpot_test_fullwiki_v1.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 116\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m files:\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\download\\download_manager.py:159\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    157\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[1;32m--> 159\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading data files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\utils\\py_utils.py:521\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[0;32m    519\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    520\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m--> 521\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    522\u001b[0m     _single_map_nested((function, obj, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[0;32m    524\u001b[0m ]\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[0;32m    526\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\utils\\py_utils.py:522\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    519\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    520\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[0;32m    521\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 522\u001b[0m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[0;32m    524\u001b[0m ]\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[0;32m    526\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\utils\\py_utils.py:390\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    385\u001b[0m     batched\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[0;32m    389\u001b[0m ):\n\u001b[1;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\utils\\py_utils.py:390\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    385\u001b[0m     batched\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[0;32m    389\u001b[0m ):\n\u001b[1;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\download\\download_manager.py:219\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[1;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[0;32m    207\u001b[0m         download_func,\n\u001b[0;32m    208\u001b[0m         url_or_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[0;32m    222\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\download\\download_manager.py:220\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[0;32m    207\u001b[0m         download_func,\n\u001b[0;32m    208\u001b[0m         url_or_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 220\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[0;32m    222\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\download\\download_manager.py:229\u001b[0m, in \u001b[0;36mDownloadManager._download_single\u001b[1;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m url_or_path_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path, url_or_filename)\n\u001b[1;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m out \u001b[38;5;241m=\u001b[39m tracked_str(out)\n\u001b[0;32m    231\u001b[0m out\u001b[38;5;241m.\u001b[39mset_origin(url_or_filename)\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\utils\\file_utils.py:214\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# Download external files\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m         output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_etag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_etag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\utils\\file_utils.py:415\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, user_agent, use_etag, token, storage_options, download_desc, disable_tqdm)\u001b[0m\n\u001b[0;32m    413\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;66;03m# GET file object\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m     \u001b[43mfsspec_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    418\u001b[0m shutil\u001b[38;5;241m.\u001b[39mmove(temp_file\u001b[38;5;241m.\u001b[39mname, cache_path)\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\datasets\\utils\\file_utils.py:339\u001b[0m, in \u001b[0;36mfsspec_get\u001b[1;34m(url, temp_file, storage_options, desc, disable_tqdm)\u001b[0m\n\u001b[0;32m    326\u001b[0m fs, path \u001b[38;5;241m=\u001b[39m url_to_fs(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[0;32m    327\u001b[0m callback \u001b[38;5;241m=\u001b[39m TqdmCallback(\n\u001b[0;32m    328\u001b[0m     tqdm_kwargs\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m: desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m     }\n\u001b[0;32m    338\u001b[0m )\n\u001b[1;32m--> 339\u001b[0m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\fsspec\\asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\fsspec\\asyn.py:91\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data:   3%|▎         | 15.3M/566M [05:00<5:33:21, 27.6kB/s] "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_and_convert_hotpotqa(output_filepath: str):\n",
    "    \"\"\"\n",
    "    Hugging Face datasets에서 HotpotQA 데이터셋을 로드하고\n",
    "    Qwen3-4B 파인튜닝 형식으로 변환하여 저장합니다.\n",
    "    'train' 스플릿을 사용하며, 질문에 대한 답변을 구성하는\n",
    "    'supporting_facts'에 해당하는 문서들만 맥락으로 포함합니다.\n",
    "\n",
    "    Args:\n",
    "        output_filepath (str): 변환된 JSON 데이터를 저장할 파일 경로\n",
    "    \"\"\"\n",
    "    print(\"HotpotQA 데이터셋을 로드합니다...\")\n",
    "    # 'hotpotqa' 데이터셋의 'fullwiki' 설정을 사용하고 'train' 스플릿을 로드\n",
    "    # datasets 라이브러리가 자동으로 다운로드 및 로드합니다.\n",
    "    try:\n",
    "        dataset = load_dataset(\"hotpot_qa\", \"fullwiki\",trust_remote_code=True)\n",
    "        print(\"데이터셋 로드 완료.\")\n",
    "        print(f\"데이터셋 크기: {len(dataset)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"HotpotQA 데이터셋 로드 중 오류 발생: {e}\")\n",
    "        print(\"데이터셋 이름이나 설정('fullwiki')을 확인해보세요.\")\n",
    "        return # 오류 발생 시 함수 종료\n",
    "\n",
    "\n",
    "    converted_data = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    print(\"데이터 변환을 시작합니다...\")\n",
    "    # 로드된 데이터셋 순회\n",
    "    for example in dataset:\n",
    "        question = example['question']         # 질문 텍스트\n",
    "        answer = example['answer']             # 최종 답변 텍스트\n",
    "        # supporting_facts는 답변을 찾기 위해 필요한 [문서 제목, 문장 인덱스] 쌍의 리스트\n",
    "        supporting_facts = example['supporting_facts']\n",
    "        # context는 모든 관련/방해 문서들의 [문서 제목, [문장들]] 리스트의 리스트\n",
    "        contexts = example['context']\n",
    "\n",
    "        # supporting_facts가 비어있는 예외적인 경우는 스킵 (HotpotQA는 보통 supporting facts가 있음)\n",
    "        if not supporting_facts:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        # supporting_facts에 해당하는 문서들만 맥락으로 추출하기 위해 필요한 문서 제목들을 집합으로 만듭니다.\n",
    "        # supporting_facts의 각 항목은 [제목, 문장 인덱스] 형태입니다.\n",
    "        supporting_titles = {title for title, sent_idx in supporting_facts}\n",
    "\n",
    "        context_text = \"\"\n",
    "        # contexts 리스트를 순회하며 supporting_titles에 해당하는 문서의 문장들을 모읍니다.\n",
    "        # contexts의 각 항목은 [문서 제목, [문장들 리스트]] 형태입니다.\n",
    "        for title, sentences in contexts:\n",
    "            if title in supporting_titles:\n",
    "                # 문서 제목과 해당 문서의 문장들을 합쳐 맥락 텍스트에 추가합니다.\n",
    "                context_text += f\"[ {title} ] \" # 문서 제목 추가 (구분용)\n",
    "                context_text += \" \".join(sentences) # 해당 문서의 모든 문장 연결\n",
    "                context_text += \"\\n\\n\" # 문서 간 구분자 추가 (모델이 문서 구분을 인지하도록 돕습니다)\n",
    "\n",
    "        # 구축된 맥락 텍스트가 비어있지 않고, 질문과 답변이 유효한 경우에만 변환\n",
    "        # HotpotQA는 기본적으로 답변 가능하며 answer 필드에 값이 있습니다.\n",
    "        if context_text.strip() and question and answer:\n",
    "            # Qwen3-4B 모델의 입력 형식 (메시지 리스트) 생성\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"맥락:\\n{context_text}\\n질문: {question}\"}, # 다중 문서 맥락임을 감안하여 줄바꿈 추가\n",
    "                {\"role\": \"assistant\", \"content\": answer}\n",
    "            ]\n",
    "            converted_data.append({\"messages\": messages})\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "            # print(f\"경고: 맥락, 질문 또는 답변이 누락된 예제 스킵: ID {example.get('id', 'ID 없음')}\")\n",
    "\n",
    "\n",
    "    # 변환된 데이터를 JSON 파일로 저장\n",
    "    output_dir = os.path.dirname(output_filepath)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(converted_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"HotpotQA 데이터 변환 완료. 총 {len(converted_data)}개의 유효한 예제가 '{output_filepath}'에 저장되었습니다.\")\n",
    "        if skipped_count > 0:\n",
    "             print(f\"처리되지 않고 스킵된 예제 {skipped_count}개 있습니다.\")\n",
    "    except Exception as e:\n",
    "         print(f\"변환된 데이터 저장 중 오류 발생: {e}\")\n",
    "\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "# 변환된 데이터를 저장할 파일 경로\n",
    "output_qwen_file_hotpotqa = 'data/hotpotqa_qwen_format.json'\n",
    "\n",
    "# 데이터 로드 및 변환 함수 실행\n",
    "try:\n",
    "    load_and_convert_hotpotqa(output_qwen_file_hotpotqa)\n",
    "except Exception as e:\n",
    "    print(f\"스크립트 실행 중 예상치 못한 오류 발생: {e}\")\n",
    "#참고: 실제 실행 시 위 try-except 블록의 주석을 해제하고 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff2e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Questions 데이터셋을 로드합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 287/287 [00:01<00:00, 233.64files/s]\n",
      "Generating train split: 100%|██████████| 307373/307373 [40:40<00:00, 125.93 examples/s]\n",
      "Generating validation split: 100%|██████████| 7830/7830 [01:00<00:00, 129.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 로드 완료.\n",
      "데이터 변환을 시작합니다...\n",
      "데이터 로드 또는 변환 중 오류 발생: 'text'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_and_convert_nq(output_filepath: str):\n",
    "    \"\"\"\n",
    "    Hugging Face datasets에서 Natural Questions (NQ) 데이터셋을 로드하고\n",
    "    Qwen3-4B 파인튜닝 형식으로 변환하여 저장합니다.\n",
    "    'open' 설정을 사용하고, 학습(train) 스플릿의 답변 가능한 예제 중\n",
    "    첫 번째 짧은 답변을 사용합니다.\n",
    "\n",
    "    Args:\n",
    "        output_filepath (str): 변환된 JSON 데이터를 저장할 파일 경로\n",
    "    \"\"\"\n",
    "    print(\"Natural Questions 데이터셋을 로드합니다...\")\n",
    "    # 'natural_questions' 데이터셋의 'open' 설정을 사용하고 'train' 스플릿을 로드\n",
    "    # validation 스플릿을 사용하려면 'train' 대신 'validation'으로 변경\n",
    "    dataset = load_dataset('natural_questions',  split='train')\n",
    "    print(\"데이터셋 로드 완료.\")\n",
    "\n",
    "    converted_data = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    print(\"데이터 변환을 시작합니다...\")\n",
    "    # 로드된 데이터셋 순회\n",
    "    for example in dataset:\n",
    "        question = example['question']['text']  # 질문 텍스트\n",
    "        context = example['document']['text']   # 문서 텍스트 (클리닝된 내용)\n",
    "        annotations = example['annotations']    # 답변 어노테이션 리스트\n",
    "\n",
    "        # 답변 가능한 예제 찾기 (짧은 답변이 하나라도 있는 경우)\n",
    "        # NQ 데이터는 여러 어노테이터의 답변이 있을 수 있으며,\n",
    "        # short_answers는 리스트 형태입니다. 여기서는 첫 번째 어노테이션의\n",
    "        # 첫 번째 short_answer를 사용하겠습니다.\n",
    "        has_answer = False\n",
    "        answer = None\n",
    "        for annotation in annotations:\n",
    "            if annotation['short_answers']: # short_answers 리스트가 비어있지 않은 경우\n",
    "                # 첫 번째 short_answer 텍스트를 사용\n",
    "                answer = annotation['short_answers'][0]['text']\n",
    "                has_answer = True\n",
    "                break # 첫 번째 유효한 답변을 찾으면 중단\n",
    "\n",
    "        if has_answer and answer: # 답변을 찾았고, 답변 텍스트가 있는 경우만 처리\n",
    "            # Qwen3-4B 모델의 입력 형식 (메시지 리스트) 생성\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"맥락: {context}\\n질문: {question}\"},\n",
    "                {\"role\": \"assistant\", \"content\": answer}\n",
    "            ]\n",
    "            converted_data.append({\"messages\": messages})\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "            # 답변이 없는 예제를 포함하고 싶다면 이 부분을 수정\n",
    "\n",
    "    # 변환된 데이터를 JSON 파일로 저장\n",
    "    output_dir = os.path.dirname(output_filepath)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"NQ 데이터 변환 완료. 총 {len(converted_data)}개의 예제가 '{output_filepath}'에 저장되었습니다.\")\n",
    "    print(f\"답변이 없거나 처리되지 않은 예제 {skipped_count}개는 스킵되었습니다.\")\n",
    "\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "# 변환된 데이터를 저장할 파일 경로\n",
    "output_qwen_file_nq = r'C:\\Users\\jiho1\\Downloads\\nq_qwen_format.json\"'\n",
    "\n",
    "# 데이터 로드 및 변환 함수 실행\n",
    "try:\n",
    "    load_and_convert_nq(output_qwen_file_nq)\n",
    "except Exception as e:\n",
    "    print(f\"데이터 로드 또는 변환 중 오류 발생: {e}\")\n",
    "\n",
    "# 참고: 실제 실행 시 위 try-except 블록의 주석을 해제하고 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651831fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiho1\\anaconda3\\envs\\llamaenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('natural_questions',  split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808f1b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD 데이터 변환 완료. 총 86821개의 예제가 'C:\\Users\\jiho1\\Downloads\\squad_qwen_format.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def convert_squad_to_qwen_format(squad_filepath: str, output_filepath: str):\n",
    "    \"\"\"\n",
    "    SQuAD 데이터셋 JSON 파일을 읽어 Qwen3-4B 파인튜닝 형식으로 변환합니다.\n",
    "\n",
    "    Args:\n",
    "        squad_filepath (str): SQuAD 데이터셋 JSON 파일 경로 (예: train-v2.0.json)\n",
    "        output_filepath (str): 변환된 JSON 데이터를 저장할 파일 경로 (예: squad_qwen_format.json)\n",
    "    \"\"\"\n",
    "    converted_data = []\n",
    "\n",
    "    # SQuAD JSON 파일 로드\n",
    "    with open(squad_filepath, 'r', encoding='utf-8') as f:\n",
    "        squad_data = json.load(f)\n",
    "\n",
    "    # 'data' 키 아래에 있는 각 문서/주제별 항목 순회\n",
    "    for entry in squad_data['data']:\n",
    "        # 각 항목의 'paragraphs' 순회\n",
    "        for paragraph in entry['paragraphs']:\n",
    "            context = paragraph['context'] # 현재 문단의 텍스트\n",
    "\n",
    "            # 현재 문단의 'qas' (질문-답변 쌍) 순회\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question'] # 질문 텍스트\n",
    "\n",
    "                # SQuAD v2.0에는 답변할 수 없는 질문이 포함될 수 있습니다.\n",
    "                # 여기서는 답변이 있는 경우만 처리하거나, 답변이 없는 경우를 고려할 수 있습니다.\n",
    "                # 기본적인 QA 능력 향상을 위해 우선 답변이 있는 경우만 처리해 보겠습니다.\n",
    "                # (v1.1 데이터셋을 사용한다면 모든 질문에 답변이 있습니다.)\n",
    "                if qa['answers']:\n",
    "                    # 여러 답변이 있을 수 있지만, 보통 첫 번째 답변을 사용합니다.\n",
    "                    answer = qa['answers'][0]['text']\n",
    "\n",
    "                    # Qwen3-4B 모델의 입력 형식 (메시지 리스트) 생성\n",
    "                    messages = [\n",
    "                        # 시스템 메시지는 필요에 따라 추가하거나 변경할 수 있습니다.\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "                        # 사용자 메시지: 맥락과 질문 포함\n",
    "                        {\"role\": \"user\", \"content\": f\"맥락: {context}\\n질문: {question}\"},\n",
    "                        # 어시스턴트 메시지: 정답 답변\n",
    "                        {\"role\": \"assistant\", \"content\": answer}\n",
    "                    ]\n",
    "\n",
    "                    converted_data.append({\"messages\": messages})\n",
    "                else:\n",
    "                    # 답변이 없는 질문을 처리하려면 이 부분을 수정해야 합니다.\n",
    "                    # 예: {\"role\": \"assistant\", \"content\": \"답변을 찾을 수 없습니다.\"}\n",
    "                    # 또는 해당 예제를 스킵. 기본적으로는 스킵합니다.\n",
    "                    pass\n",
    "\n",
    "\n",
    "    # 변환된 데이터를 JSON 파일로 저장\n",
    "    output_dir = os.path.dirname(output_filepath)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "        # JSON 배열 형태로 저장, ensure_ascii=False로 한글 깨짐 방지\n",
    "        json.dump(converted_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"SQuAD 데이터 변환 완료. 총 {len(converted_data)}개의 예제가 '{output_filepath}'에 저장되었습니다.\")\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "# SQuAD 학습 데이터 파일 경로 (실제 파일 경로로 수정하세요)\n",
    "squad_input_file = 'train-v2.0.json' # 또는 train-v2.0.json\n",
    "# 변환된 데이터를 저장할 파일 경로\n",
    "output_qwen_file = r\"C:\\Users\\jiho1\\Downloads\\squad_qwen_format.json\"\n",
    "\n",
    "\n",
    "# 데이터 변환 함수 실행\n",
    "try:\n",
    "    convert_squad_to_qwen_format(squad_input_file, output_qwen_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: SQuAD 파일을 찾을 수 없습니다. 경로를 확인하세요: {squad_input_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"변환 중 오류 발생: {e}\")\n",
    "\n",
    "# 참고: 실제 실행 시 위 try-except 블록의 주석을 해제하고 파일 경로를 올바르게 설정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6f0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('KorQuAD_v1.0_train.json','r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6813c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD 데이터 변환 완료. 총 50000개의 예제가 'C:\\Users\\jiho1\\Downloads\\korquad_qwen_format_re.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def convert_squad_to_qwen_format(squad_filepath: str, output_filepath: str):\n",
    "    \"\"\"\n",
    "    SQuAD 데이터셋 JSON 파일을 읽어 Qwen3-4B 파인튜닝 형식으로 변환합니다.\n",
    "\n",
    "    Args:\n",
    "        squad_filepath (str): SQuAD 데이터셋 JSON 파일 경로 (예: train-v2.0.json)\n",
    "        output_filepath (str): 변환된 JSON 데이터를 저장할 파일 경로 (예: squad_qwen_format.json)\n",
    "    \"\"\"\n",
    "    converted_data = []\n",
    "\n",
    "    # SQuAD JSON 파일 로드\n",
    "    with open(squad_filepath, 'r', encoding='utf-8') as f:\n",
    "        squad_data = json.load(f)\n",
    "\n",
    "    # 'data' 키 아래에 있는 각 문서/주제별 항목 순회\n",
    "    for entry in squad_data['data']:\n",
    "        # 각 항목의 'paragraphs' 순회\n",
    "        for paragraph in entry['paragraphs']:\n",
    "            context = paragraph['context'] # 현재 문단의 텍스트\n",
    "\n",
    "            # 현재 문단의 'qas' (질문-답변 쌍) 순회\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question'] # 질문 텍스트\n",
    "\n",
    "                # SQuAD v2.0에는 답변할 수 없는 질문이 포함될 수 있습니다.\n",
    "                # 여기서는 답변이 있는 경우만 처리하거나, 답변이 없는 경우를 고려할 수 있습니다.\n",
    "                # 기본적인 QA 능력 향상을 위해 우선 답변이 있는 경우만 처리해 보겠습니다.\n",
    "                # (v1.1 데이터셋을 사용한다면 모든 질문에 답변이 있습니다.)\n",
    "                if qa['answers']:\n",
    "                    # 여러 답변이 있을 수 있지만, 보통 첫 번째 답변을 사용합니다.\n",
    "                    answer = qa['answers'][0]['text']\n",
    "\n",
    "                    # Qwen3-4B 모델의 입력 형식 (메시지 리스트) 생성\n",
    "                    messages = [\n",
    "                        # 시스템 메시지는 필요에 따라 추가하거나 변경할 수 있습니다.\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "                        # 사용자 메시지: 맥락과 질문 포함\n",
    "                        {\"role\": \"user\", \"content\": f\"맥락: {context}\\n질문: {question}\"},\n",
    "                        # 어시스턴트 메시지: 정답 답변\n",
    "                        {\"role\": \"assistant\", \"content\": answer}\n",
    "                    ]\n",
    "\n",
    "                    converted_data.append({\"messages\": messages})\n",
    "                    if len(converted_data) == 50000:\n",
    "                        break\n",
    "                else:\n",
    "                    # 답변이 없는 질문을 처리하려면 이 부분을 수정해야 합니다.\n",
    "                    # 예: {\"role\": \"assistant\", \"content\": \"답변을 찾을 수 없습니다.\"}\n",
    "                    # 또는 해당 예제를 스킵. 기본적으로는 스킵합니다.\n",
    "                    pass\n",
    "            if len(converted_data) == 50000:\n",
    "                break\n",
    "        if len(converted_data) == 50000:\n",
    "            break\n",
    "\n",
    "\n",
    "    # 변환된 데이터를 JSON 파일로 저장\n",
    "    output_dir = os.path.dirname(output_filepath)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "        # JSON 배열 형태로 저장, ensure_ascii=False로 한글 깨짐 방지\n",
    "        json.dump(converted_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"SQuAD 데이터 변환 완료. 총 {len(converted_data)}개의 예제가 '{output_filepath}'에 저장되었습니다.\")\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "# SQuAD 학습 데이터 파일 경로 (실제 파일 경로로 수정하세요)\n",
    "squad_input_file = 'KorQuAD_v1.0_train.json' # 또는 train-v2.0.json\n",
    "# 변환된 데이터를 저장할 파일 경로\n",
    "output_qwen_file = r\"C:\\Users\\jiho1\\Downloads\\korquad_qwen_format_re.json\"\n",
    "\n",
    "\n",
    "# 데이터 변환 함수 실행\n",
    "try:\n",
    "    convert_squad_to_qwen_format(squad_input_file, output_qwen_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: SQuAD 파일을 찾을 수 없습니다. 경로를 확인하세요: {squad_input_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"변환 중 오류 발생: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
