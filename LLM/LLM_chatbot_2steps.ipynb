{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32cb02ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'Qwen/Qwen3-4B'를 로드합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD 파인튜닝된 LoRA 어댑터 './qwen_squad_finetuned/squad_lora_adapter'를 로드하여 기본 모델에 연결합니다...\n",
      "SQuAD 어댑터 로드 및 연결 완료.\n",
      "모델 및 토크나이저 로드 완료.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer \n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel # QLoRA를 위한 준비 함수\n",
    "\n",
    "# --- 모델 및 토크나이저 로드 ---\n",
    "# Qwen1.5-4B 모델을 사용합니다. 정확한 모델 ID를 Hugging Face Hub에서 확인해 주세요.\n",
    "# Qwen/Qwen1.5-4B-Chat 버전으로도 시도해 볼 수 있습니다.\n",
    "model_id = \"Qwen/Qwen3-4B\" \n",
    "\n",
    "print(f\"모델 '{model_id}'를 로드합니다...\")\n",
    "# 모델 로드 (GPU 메모리 절약을 위해 torch_dtype을 bfloat16으로 설정)\n",
    "# Colab Pro 또는 고성능 GPU가 아니라면 4bit 양자화를 고려해 볼 수 있습니다.\n",
    "# 만약 4bit 양자화를 사용하려면 BitsAndBytesConfig를 임포트하고 적용해야 합니다.\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ") \n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16, # BFloat16으로 로드 (GPU가 지원할 경우)\n",
    "    device_map=\"auto\",          # 사용 가능한 디바이스에 자동으로 매핑\n",
    "    quantization_config=bnb_config # 4bit 양자화 사용 시 주석 해제\n",
    ")\n",
    "\n",
    "squad_finetuned_adapter_path = './qwen_squad_finetuned/squad_lora_adapter'\n",
    "print(f\"SQuAD 파인튜닝된 LoRA 어댑터 '{squad_finetuned_adapter_path}'를 로드하여 기본 모델에 연결합니다...\")\n",
    "# 기본 모델에 SQuAD 파인튜닝 어댑터를 연결합니다.\n",
    "# 이제 'model' 객체는 SQuAD로 학습된 QA 능력을 가진 상태가 됩니다.\n",
    "model = PeftModel.from_pretrained(base_model, squad_finetuned_adapter_path, trust_remote_code=True)\n",
    "model.train() # 학습 모드로 설정 (SFTTrainer가 다시 설정해주지만 명시적으로)\n",
    "print(\"SQuAD 어댑터 로드 및 연결 완료.\")\n",
    "# QLoRA 사용 시 모델을 K-bit 학습에 맞게 준비\n",
    "model = prepare_model_for_kbit_training(model) # 4bit 양자화 사용 시 주석 해제\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# 토크나이저 패딩 토큰 설정 (Qwen 모델은 일반적으로 pad_token이 명시적으로 설정되어 있지 않으므로 eos_token을 사용)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # 패딩 방향 설정: 주로 오른쪽 패딩이 선호됨\n",
    "\n",
    "print(\"모델 및 토크나이저 로드 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d19982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 'korquad_qwen_format_processed.json'를 로드합니다...\n",
      "데이터셋 로드 완료. 총 50000개의 예제.\n",
      "학습 데이터셋 크기: 47500, 검증 데이터셋 크기: 2500\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- 데이터셋 로드 및 준비 ---\n",
    "# 변환된 SQuAD 데이터셋 파일 경로 (2번 섹션에서 저장한 파일)\n",
    "squad_qwen_format_path = 'korquad_qwen_format_processed.json'\n",
    "\n",
    "print(f\"데이터셋 '{squad_qwen_format_path}'를 로드합니다...\")\n",
    "with open(squad_qwen_format_path, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Hugging Face datasets 라이브러리의 Dataset 객체로 변환\n",
    "dataset = Dataset.from_list(raw_data[:50000])\n",
    "print(f\"데이터셋 로드 완료. 총 {len(dataset)}개의 예제.\")\n",
    "\n",
    "# 데이터셋을 학습 세트와 검증 세트로 분할 (전체 데이터의 5%를 검증 세트로 사용)\n",
    "# seed 값을 고정하여 매번 동일한 분할 결과 얻기\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "print(f\"학습 데이터셋 크기: {len(train_dataset)}, 검증 데이터셋 크기: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b356aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen3ForCausalLM(\n",
      "      (model): Qwen3Model(\n",
      "        (embed_tokens): Embedding(151936, 2560)\n",
      "        (layers): ModuleList(\n",
      "          (0-35): 36 x Qwen3DecoderLayer(\n",
      "            (self_attn): Qwen3Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "            )\n",
      "            (mlp): Qwen3MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=9728, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=9728, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "        (rotary_emb): Qwen3RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad68f754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.training_args\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "print(TrainingArguments.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc85b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.trainer\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "print(Trainer.__module__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a534f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파인튜닝 설정을 정의합니다...\n",
      "파인튜닝 설정 완료.\n",
      "LoRA 설정을 정의합니다...\n",
      "LoRA 설정 완료.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "# --- 파인튜닝 설정 (TrainingArguments) ---\n",
    "output_dir = \"./qwen_korquad_finetuned\" # 학습된 모델이 저장될 경로\n",
    "\n",
    "print(\"파인튜닝 설정을 정의합니다...\")\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,      # GPU당 학습 배치 크기 (메모리 제약에 따라 조절)\n",
    "    gradient_accumulation_steps=4,      # 그라디언트 누적 단계 (실제 배치 크기 = 2 * 4 = 8)\n",
    "    learning_rate=2e-4,                 # 학습률\n",
    "    num_train_epochs=3,                 # 학습 에포크 수\n",
    "    logging_steps=100,                  # 몇 스텝마다 로그를 출력할지\n",
    "    save_steps=500,                     # 몇 스텝마다 모델 체크포인트를 저장할지\n",
    "    eval_strategy=\"steps\",        # 스텝 단위로 검증 수행\n",
    "    eval_steps=500,                     # 몇 스텝마다 검증을 수행할지\n",
    "    save_total_limit=3,                 # 저장할 체크포인트 최대 개수\n",
    "    fp16=True,                          # Float16 정밀도 학습 (GPU 지원 시 메모리 절약)\n",
    "    report_to=\"none\",                   # 학습 진행 상황 리포트 (wandb 등. 'none' 시 로컬 로그만 사용)\n",
    "    load_best_model_at_end=True,        # 학습 종료 시 가장 성능이 좋았던 모델 로드\n",
    "    metric_for_best_model=\"eval_loss\",  # 최적 모델을 결정할 지표 (검증 손실이 가장 낮은 모델)\n",
    ")\n",
    "\n",
    "print(\"파인튜닝 설정 완료.\")\n",
    "\n",
    "# --- LoRA 설정 (PEFT) ---\n",
    "print(\"LoRA 설정을 정의합니다...\")\n",
    "# r: LoRA의 랭크 (낮은 값은 더 작은 어댑터, 높은 값은 더 큰 어댑터)\n",
    "# lora_alpha: LoRA 스케일링 팩터\n",
    "# target_modules: LoRA를 적용할 모델의 모듈 이름. Qwen1.5 모델에 맞게 'q_proj', 'k_proj', 'v_proj', 'o_proj'를 일반적으로 타겟으로 합니다.\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # LoRA 랭크 (일반적으로 8, 16, 32, 64)\n",
    "    lora_alpha=16, # LoRA 스케일링 계수\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Qwen1.5의 주요 선형 레이어\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\", # 텍스트 생성 (인과적 언어 모델링)\n",
    ")\n",
    "print(\"LoRA 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c4add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "\n",
    "# numpy._core.multiarray._reconstruct를 안전 목록에 추가\n",
    "add_safe_globals([np._core.multiarray._reconstruct])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801a30db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': '당신은 제공된 맥락에 따라 질문에 답하는 유용한 AI입니다. 반드시 한국어로 답 해주세요.',\n",
       "   'role': 'system'},\n",
       "  {'content': '맥락: 즉 이황의 설은 호발설이고 이이는 일도설이었다. 하지만 이황의 칠정설인 기발이승설만을 취한 것이다. 반면에 그의 사단설인 이발기수(理發氣隨)설을 비판하였다. 또한 그것은 이황과 사단칠정설논쟁을 벌인 기대승의 영향을 받은 것이다. 또한 서경덕에 대해서도 기중심의 설로서 독창적이지만 문제가 있다며 비판한다. 그의 설은 기가 운동하고 이는 그 원인이 된다는 설에 근거한 것이다. 그는 자기의 주장을 발전시키면서 이 주장이 주자의 뜻과 어긋나면 주자가 잘못 된 것이라고까지 하는 자신을 얻게 된 것이다. 이같이 그는 학문으로 유명할 뿐 아니라 경세가(經世家)로서도 혁혁한 업적을 남겼다. 훗날 영남의 유직이 효종원년에 올린 상소문에서 이이의 설을 불교와 육왕(육구연과 왕수인)과 같은 주기설로서 이단이라고 공격하였다. 그는 이황이야말로 주리설로서 정학이라는 사상에 근거하여 이이를 비판했던 것이다. 그후 주리설은 정학이고 주기설은 이단이라고 여겨지게 되었다. 그 상소로 인하여 유직은 조정으로부터 처벌받아 과거시험을 응시할 수 있는 자격을 박탈당하였다. 하지만 이이 역시 기의 뿌리가 이라고 말하였기 때문에 주리설이라고 할 수 있다. 또한 이황과 이이 모두 기의 뿌리가 리라고 했기 때문에 모두 이일원론 또는 이기일원론이라고 할 수 있다. 그의 저작인 《동호문답(東湖問答)》, 《성학집요(聖學輯要)》, 《인심도심설(人心道心說)》, 《시무육조소(時務六條疏)》 등은 모두 임금의 도리와 시무를 논한 명저로 그의 정치에 대한 태도는 유학자의 이상인 요순시대를 실현하는 것이었다.\\n질문: 이이는 이황의 칠정성인 기발이승설은 취했는데. 반면에 비판한 사단설은 무엇인가?',\n",
       "   'role': 'user'},\n",
       "  {'content': '그것은 바로 이발기수(理發氣隨)설이랍니다.', 'role': 'assistant'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "c:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\peft\\peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTrainer를 초기화합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML: 100%|██████████| 47500/47500 [00:04<00:00, 9585.93 examples/s] \n",
      "Applying chat template to train dataset: 100%|██████████| 47500/47500 [00:10<00:00, 4406.03 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 47500/47500 [00:43<00:00, 1095.36 examples/s]\n",
      "Packing train dataset: 100%|██████████| 47500/47500 [00:00<00:00, 1051131.91 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 2500/2500 [00:00<00:00, 9748.55 examples/s] \n",
      "Applying chat template to eval dataset: 100%|██████████| 2500/2500 [00:00<00:00, 4247.34 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 2500/2500 [00:02<00:00, 1015.61 examples/s]\n",
      "Packing eval dataset: 100%|██████████| 2500/2500 [00:00<00:00, 356816.28 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTrainer 초기화 완료. 학습을 시작합니다.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# 기존 어댑터 로드\n",
    "peft_config = PeftConfig.from_pretrained(r\".\\qwen_squad_finetuned\\checkpoint-3000\")\n",
    "model = PeftModel.from_pretrained(model, r\".\\qwen_squad_finetuned\\checkpoint-3000\",is_trainable=True)\n",
    "\n",
    "print(\"SFTTrainer를 초기화합니다...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, # 검증 데이터셋 추가\n",
    "    peft_config=lora_config,\n",
    "    #args=training_arguments,\n",
    "    # False로 설정 시 각 예제가 독립적으로 처리됩니다.\n",
    "    # SQuAD 맥락과 질문, 답변을 합친 길이가 이 값을 넘으면 잘려나갈 수 있습니다.\n",
    "    args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4, \n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        dataset_text_field=\"text\",\n",
    "        eval_strategy=\"steps\", \n",
    "        eval_steps=500,    \n",
    "        save_total_limit=3,\n",
    "        disable_tqdm=False,\n",
    "        fp16=True,\n",
    "        report_to=\"none\", \n",
    "        max_seq_length=512,\n",
    "        load_best_model_at_end=True,\n",
    "        packing=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        resume_from_checkpoint=True\n",
    "    )\n",
    "    \n",
    ")\n",
    "print(\"SFTTrainer 초기화 완료. 학습을 시작합니다.\")\n",
    "\n",
    "# 모델 학습 시작\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9910d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='8055' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/8055 00:22 < 50:02:02, 0.04 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\transformers\\trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\transformers\\trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2553\u001b[0m )\n\u001b[0;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2561\u001b[0m ):\n\u001b[0;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\transformers\\trainer.py:3791\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3789\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\accelerate\\accelerator.py:2469\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0137df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 LoRA 어댑터가 './qwen_squad_finetuned\\squad_lora_adapter'에 저장되었습니다.\n",
      "이제 저장된 어댑터와 기본 모델을 병합하여 새로운 모델을 만들거나, 어댑터를 로드하여 추론에 사용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 학습된 LoRA 어댑터 저장\n",
    "output_adapter_dir = os.path.join(output_dir, \"korquad_lora_adapter\")\n",
    "trainer.model.save_pretrained(output_adapter_dir)\n",
    "tokenizer.save_pretrained(output_adapter_dir) # 토크나이저도 함께 저장\n",
    "\n",
    "print(f\"학습된 LoRA 어댑터가 '{output_adapter_dir}'에 저장되었습니다.\")\n",
    "print(\"이제 저장된 어댑터와 기본 모델을 병합하여 새로운 모델을 만들거나, 어댑터를 로드하여 추론에 사용할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df17ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재까지 학습된 LoRA 어댑터를 './qwen_squad_finetuned\\manual_save_20250527-103754'에 저장합니다...\n",
      "현재 학습 상태가 './qwen_squad_finetuned\\manual_save_20250527-103754'에 성공적으로 저장되었습니다.\n",
      "이 어댑터를 로드하여 추론하거나, 나중에 학습을 재개할 때 사용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 이 코드를 주피터 노트북의 새로운 코드 셀에 넣어 실행하세요.\n",
    "# trainer.train()을 중단한 직후에 실행하면 됩니다.\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 저장할 디렉토리 경로 정의 (현재 시간으로 고유한 이름 생성)\n",
    "# training_arguments.output_dir은 이전에 설정했던 'qwen_squad_finetuned'입니다.\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "manual_save_dir = os.path.join(training_arguments.output_dir, f\"manual_save_{timestamp}\")\n",
    "\n",
    "print(f\"현재까지 학습된 LoRA 어댑터를 '{manual_save_dir}'에 저장합니다...\")\n",
    "\n",
    "# 저장할 디렉토리가 없으면 생성\n",
    "if not os.path.exists(manual_save_dir):\n",
    "    os.makedirs(manual_save_dir)\n",
    "\n",
    "# trainer 객체에서 현재 모델의 LoRA 어댑터 상태를 저장\n",
    "# trainer.model은 PEFT (LoRA) 모델 객체입니다.\n",
    "trainer.model.save_pretrained(manual_save_dir)\n",
    "\n",
    "# 토크나이저도 함께 저장하여 나중에 이 어댑터를 로드할 때 사용할 수 있도록 합니다.\n",
    "tokenizer.save_pretrained(manual_save_dir)\n",
    "\n",
    "print(f\"현재 학습 상태가 '{manual_save_dir}'에 성공적으로 저장되었습니다.\")\n",
    "print(\"이 어댑터를 로드하여 추론하거나, 나중에 학습을 재개할 때 사용할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:22<00:00, 27.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어댑터 로드 및 연결 완료.\n",
      "\n",
      "--- 테스트 추론 ---\n",
      "질문: When did Beyonce become famous?\n",
      "맥락: Beyoncé Giselle Knowles-Carter (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\n",
      "답변: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "late 1990s\n",
      "\n",
      "--- LoRA 어댑터 병합 시작 (선택 사항) ---\n",
      "어댑터 병합 완료.\n",
      "병합된 모델이 './qwen_squad_finetuned_merged'에 저장되었습니다.\n",
      "작업 완료.\n"
     ]
    }
   ],
   "source": [
    "# --- 학습된 모델로 추론하기 (선택 사항) ---\n",
    "from peft import PeftModel\n",
    "from transformers import pipeline\n",
    "\n",
    "# 저장된 어댑터 로드\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config=bnb_config # 4bit 양자화 사용 시 주석 해제\n",
    ")\n",
    "\n",
    "# 어댑터를 기본 모델에 연결\n",
    "adapter_path = os.path.join(output_dir, \"korquad_lora_adapter\")\n",
    "model_with_adapter = PeftModel.from_pretrained(loaded_model, adapter_path)\n",
    "print(\"어댑터 로드 및 연결 완료.\")\n",
    "\n",
    "# 추론을 위한 파이프라인 생성 (Qwen 토크나이저 사용)\n",
    "# Qwen 모델은 채팅 템플릿 적용이 필요합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "def generate_answer(question_text, context_text, model, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"맥락: {context_text}\\n질문: {question_text}\"}\n",
    "    ]\n",
    "    # 채팅 템플릿 적용\n",
    "    # 이 부분은 Qwen1.5 모델의 최신 채팅 템플릿에 따라 약간 달라질 수 있습니다.\n",
    "    # 일반적으로 apply_chat_template을 사용합니다.\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # 입력 프롬프트 부분을 제외하고 생성된 텍스트만 디코딩\n",
    "    generated_text = tokenizer.decode(generated_ids[0][model_inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "# 예시 질문과 맥락\n",
    "test_context = \"Beyoncé Giselle Knowles-Carter (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\"\n",
    "test_question = \"When did Beyonce become famous?\"\n",
    "\n",
    "print(\"\\n--- 테스트 추론 ---\")\n",
    "print(f\"질문: {test_question}\")\n",
    "print(f\"맥락: {test_context}\")\n",
    "answer = generate_answer(test_question, test_context, model_with_adapter, tokenizer)\n",
    "print(f\"답변: {answer}\")\n",
    "\n",
    "\n",
    "# --- LoRA 어댑터와 기본 모델 병합 및 저장 (선택 사항) ---\n",
    "# 파인튜닝된 모델을 일반적인 형태로 저장하여 쉽게 배포하거나 사용할 수 있도록 합니다.\n",
    "# GPU 메모리가 충분해야 이 작업을 수행할 수 있습니다.\n",
    "# qlora를 사용했다면, merge_and_unload() 전에 prepare_model_for_kbit_training을 해제해야 할 수 있습니다.\n",
    "\n",
    "print(\"\\n--- LoRA 어댑터 병합 시작 (선택 사항) ---\")\n",
    "try:\n",
    "    # QLoRA를 사용했다면 model_with_adapter.merge_and_unload()를 사용하기 전에\n",
    "    # 모델을 CPU로 이동시키거나 (model_with_adapter.to('cpu'))\n",
    "    # 일부 설정을 비활성화해야 할 수 있습니다.\n",
    "    # 일반적인 LoRA에서는 바로 merge_and_unload()를 사용합니다.\n",
    "    merged_model = model_with_adapter.merge_and_unload()\n",
    "    print(\"어댑터 병합 완료.\")\n",
    "\n",
    "    # 병합된 모델 저장\n",
    "    merged_model_output_dir = \"./qwen_squad_finetuned_merged\"\n",
    "    merged_model.save_pretrained(merged_model_output_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(merged_model_output_dir)\n",
    "    print(f\"병합된 모델이 '{merged_model_output_dir}'에 저장되었습니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"모델 병합 중 오류 발생: {e}\")\n",
    "    print(\"GPU 메모리 부족이 원인일 수 있습니다. 병합은 많은 메모리를 요구합니다.\")\n",
    "\n",
    "print(\"작업 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- LoRA 어댑터 병합 시작 (선택 사항) ---\")\n",
    "try:\n",
    "    # QLoRA를 사용했다면 model_with_adapter.merge_and_unload()를 사용하기 전에\n",
    "    # 모델을 CPU로 이동시키거나 (model_with_adapter.to('cpu'))\n",
    "    # 일부 설정을 비활성화해야 할 수 있습니다.\n",
    "    # 일반적인 LoRA에서는 바로 merge_and_unload()를 사용합니다.\n",
    "    merged_model = model_with_adapter.merge_and_unload()\n",
    "    print(\"어댑터 병합 완료.\")\n",
    "\n",
    "    # 병합된 모델 저장\n",
    "    merged_model_output_dir = \"./qwen_squad_finetuned_merged\"\n",
    "    merged_model.save_pretrained(merged_model_output_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(merged_model_output_dir)\n",
    "    print(f\"병합된 모델이 '{merged_model_output_dir}'에 저장되었습니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"모델 병합 중 오류 발생: {e}\")\n",
    "    print(\"GPU 메모리 부족이 원인일 수 있습니다. 병합은 많은 메모리를 요구합니다.\")\n",
    "\n",
    "print(\"작업 완료.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
