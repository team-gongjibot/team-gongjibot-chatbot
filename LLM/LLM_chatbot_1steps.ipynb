{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32cb02ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1372\u001b[0m, in \u001b[0;36m_path_importer_cache\u001b[1;34m(cls, path)\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'c:\\\\Users\\\\jiho1\\\\anaconda3\\\\envs\\\\extra_llmenv\\\\lib\\\\site-packages\\\\torch\\\\distributed'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prepare_model_for_kbit_training \u001b[38;5;66;03m# QLoRA를 위한 준비 함수\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\__init__.py:2016\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2011\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \n\u001b[0;32m   2015\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[1;32m-> 2016\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2020\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[0;32m   2021\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\functional.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\nn\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[0;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[0;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[0;32m      6\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[0;32m     11\u001b[0m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\nn\\modules\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     CELU,\n\u001b[0;32m      5\u001b[0m     ELU,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     Threshold,\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeviceLikeType\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Buffer, Parameter\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackwardHook, RemovableHandle\n\u001b[0;32m     33\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_pre_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\utils\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     backcompat \u001b[38;5;28;01mas\u001b[39;00m backcompat,\n\u001b[0;32m     10\u001b[0m     collect_env \u001b[38;5;28;01mas\u001b[39;00m collect_env,\n\u001b[0;32m     11\u001b[0m     data \u001b[38;5;28;01mas\u001b[39;00m data,\n\u001b[0;32m     12\u001b[0m     deterministic \u001b[38;5;28;01mas\u001b[39;00m deterministic,\n\u001b[0;32m     13\u001b[0m     hooks \u001b[38;5;28;01mas\u001b[39;00m hooks,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_registration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     generate_methods_for_privateuse1_backend,\n\u001b[0;32m     17\u001b[0m     rename_privateuse1_backend,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_backtrace\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_cpp_backtrace\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\utils\\data\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     _DatasetKind,\n\u001b[0;32m      3\u001b[0m     DataLoader,\n\u001b[0;32m      4\u001b[0m     default_collate,\n\u001b[0;32m      5\u001b[0m     default_convert,\n\u001b[0;32m      6\u001b[0m     get_worker_info,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     argument_validation,\n\u001b[0;32m     10\u001b[0m     functional_datapipe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     runtime_validation_disabled,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     DataChunk,\n\u001b[0;32m     18\u001b[0m     DFIterDataPipe,\n\u001b[0;32m     19\u001b[0m     IterDataPipe,\n\u001b[0;32m     20\u001b[0m     MapDataPipe,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Generic, Iterable, List, Optional, TypeVar, Union\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_settings\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExceptionWrapper\n",
      "File \u001b[1;32mc:\\Users\\jiho1\\anaconda3\\envs\\extra_llmenv\\lib\\site-packages\\torch\\distributed\\__init__.py:122\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distributed_c10d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HashStore\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_mesh\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeviceMesh, init_device_mesh\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Variables prefixed with underscore are not auto imported\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# See the comment in `distributed_c10d.py` above `_backend` on why we expose\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# this.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed_c10d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1408\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1374\u001b[0m, in \u001b[0;36m_path_importer_cache\u001b[1;34m(cls, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1350\u001b[0m, in \u001b[0;36m_path_hooks\u001b[1;34m(path)\u001b[0m\n",
      "File \u001b[1;32m<frozen zipimport>:76\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import prepare_model_for_kbit_training # QLoRA를 위한 준비 함수\n",
    "\n",
    "# --- 모델 및 토크나이저 로드 ---\n",
    "# Qwen1.5-4B 모델을 사용합니다. 정확한 모델 ID를 Hugging Face Hub에서 확인해 주세요.\n",
    "# Qwen/Qwen1.5-4B-Chat 버전으로도 시도해 볼 수 있습니다.\n",
    "model_id = \"Qwen/Qwen3-4B\" \n",
    "\n",
    "print(f\"모델 '{model_id}'를 로드합니다...\")\n",
    "# 모델 로드 (GPU 메모리 절약을 위해 torch_dtype을 bfloat16으로 설정)\n",
    "# Colab Pro 또는 고성능 GPU가 아니라면 4bit 양자화를 고려해 볼 수 있습니다.\n",
    "# 만약 4bit 양자화를 사용하려면 BitsAndBytesConfig를 임포트하고 적용해야 합니다.\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ") \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16, # BFloat16으로 로드 (GPU가 지원할 경우)\n",
    "    device_map=\"auto\",          # 사용 가능한 디바이스에 자동으로 매핑\n",
    "    quantization_config=bnb_config # 4bit 양자화 사용 시 주석 해제\n",
    ")\n",
    "\n",
    "# QLoRA 사용 시 모델을 K-bit 학습에 맞게 준비\n",
    "model = prepare_model_for_kbit_training(model) # 4bit 양자화 사용 시 주석 해제\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# 토크나이저 패딩 토큰 설정 (Qwen 모델은 일반적으로 pad_token이 명시적으로 설정되어 있지 않으므로 eos_token을 사용)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # 패딩 방향 설정: 주로 오른쪽 패딩이 선호됨\n",
    "\n",
    "print(\"모델 및 토크나이저 로드 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d19982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 'squad_qwen_format.json'를 로드합니다...\n",
      "데이터셋 로드 완료. 총 50000개의 예제.\n",
      "학습 데이터셋 크기: 47500, 검증 데이터셋 크기: 2500\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- 데이터셋 로드 및 준비 ---\n",
    "# 변환된 SQuAD 데이터셋 파일 경로 (2번 섹션에서 저장한 파일)\n",
    "squad_qwen_format_path = 'squad_qwen_format.json'\n",
    "\n",
    "print(f\"데이터셋 '{squad_qwen_format_path}'를 로드합니다...\")\n",
    "with open(squad_qwen_format_path, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Hugging Face datasets 라이브러리의 Dataset 객체로 변환\n",
    "dataset = Dataset.from_list(raw_data[:50000])\n",
    "print(f\"데이터셋 로드 완료. 총 {len(dataset)}개의 예제.\")\n",
    "\n",
    "# 데이터셋을 학습 세트와 검증 세트로 분할 (전체 데이터의 5%를 검증 세트로 사용)\n",
    "# seed 값을 고정하여 매번 동일한 분할 결과 얻기\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "print(f\"학습 데이터셋 크기: {len(train_dataset)}, 검증 데이터셋 크기: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b356aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 2560)\n",
      "    (layers): ModuleList(\n",
      "      (0-35): 36 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad68f754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.training_args\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "print(TrainingArguments.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc85b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.trainer\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "print(Trainer.__module__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a534f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파인튜닝 설정을 정의합니다...\n",
      "파인튜닝 설정 완료.\n",
      "LoRA 설정을 정의합니다...\n",
      "LoRA 설정 완료.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "# --- 파인튜닝 설정 (TrainingArguments) ---\n",
    "output_dir = \"./qwen_squad_finetuned\" # 학습된 모델이 저장될 경로\n",
    "\n",
    "print(\"파인튜닝 설정을 정의합니다...\")\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,      # GPU당 학습 배치 크기 (메모리 제약에 따라 조절)\n",
    "    gradient_accumulation_steps=4,      # 그라디언트 누적 단계 (실제 배치 크기 = 2 * 4 = 8)\n",
    "    learning_rate=2e-4,                 # 학습률\n",
    "    num_train_epochs=3,                 # 학습 에포크 수\n",
    "    logging_steps=100,                  # 몇 스텝마다 로그를 출력할지\n",
    "    save_steps=500,                     # 몇 스텝마다 모델 체크포인트를 저장할지\n",
    "    eval_strategy=\"steps\",        # 스텝 단위로 검증 수행\n",
    "    eval_steps=500,                     # 몇 스텝마다 검증을 수행할지\n",
    "    save_total_limit=3,                 # 저장할 체크포인트 최대 개수\n",
    "    fp16=True,                          # Float16 정밀도 학습 (GPU 지원 시 메모리 절약)\n",
    "    report_to=\"none\",                   # 학습 진행 상황 리포트 (wandb 등. 'none' 시 로컬 로그만 사용)\n",
    "    load_best_model_at_end=True,        # 학습 종료 시 가장 성능이 좋았던 모델 로드\n",
    "    metric_for_best_model=\"eval_loss\",  # 최적 모델을 결정할 지표 (검증 손실이 가장 낮은 모델)\n",
    ")\n",
    "\n",
    "print(\"파인튜닝 설정 완료.\")\n",
    "\n",
    "# --- LoRA 설정 (PEFT) ---\n",
    "print(\"LoRA 설정을 정의합니다...\")\n",
    "# r: LoRA의 랭크 (낮은 값은 더 작은 어댑터, 높은 값은 더 큰 어댑터)\n",
    "# lora_alpha: LoRA 스케일링 팩터\n",
    "# target_modules: LoRA를 적용할 모델의 모듈 이름. Qwen1.5 모델에 맞게 'q_proj', 'k_proj', 'v_proj', 'o_proj'를 일반적으로 타겟으로 합니다.\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # LoRA 랭크 (일반적으로 8, 16, 32, 64)\n",
    "    lora_alpha=16, # LoRA 스케일링 계수\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Qwen1.5의 주요 선형 레이어\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\", # 텍스트 생성 (인과적 언어 모델링)\n",
    ")\n",
    "print(\"LoRA 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c4add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.serialization import add_safe_globals\n",
    "\n",
    "# numpy._core.multiarray._reconstruct를 안전 목록에 추가\n",
    "add_safe_globals([np._core.multiarray._reconstruct])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiho1\\anaconda3\\envs\\llmenv\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTrainer를 초기화합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML: 100%|██████████| 47500/47500 [00:04<00:00, 9787.69 examples/s] \n",
      "Applying chat template to train dataset: 100%|██████████| 47500/47500 [00:10<00:00, 4450.80 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 47500/47500 [00:29<00:00, 1609.10 examples/s]\n",
      "Packing train dataset: 100%|██████████| 47500/47500 [00:00<00:00, 1010270.79 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 2500/2500 [00:00<00:00, 9557.79 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 2500/2500 [00:00<00:00, 4128.52 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 2500/2500 [00:01<00:00, 1402.41 examples/s]\n",
      "Packing eval dataset: 100%|██████████| 2500/2500 [00:00<?, ? examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTrainer 초기화 완료. 학습을 시작합니다.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# 기존 어댑터 로드\n",
    "peft_config = PeftConfig.from_pretrained(r\".\\qwen_squad_finetuned\\checkpoint-3000\")\n",
    "model = PeftModel.from_pretrained(model, r\".\\qwen_squad_finetuned\\checkpoint-3000\",is_trainable=True)\n",
    "\n",
    "print(\"SFTTrainer를 초기화합니다...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, # 검증 데이터셋 추가\n",
    "    peft_config=lora_config,\n",
    "    #args=training_arguments,\n",
    "    # False로 설정 시 각 예제가 독립적으로 처리됩니다.\n",
    "    # SQuAD 맥락과 질문, 답변을 합친 길이가 이 값을 넘으면 잘려나갈 수 있습니다.\n",
    "    args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4, \n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        dataset_text_field=\"text\",\n",
    "        eval_strategy=\"steps\", \n",
    "        eval_steps=500,    \n",
    "        save_total_limit=3,\n",
    "        disable_tqdm=False,\n",
    "        fp16=True,\n",
    "        report_to=\"none\", \n",
    "        max_seq_length=512,\n",
    "        load_best_model_at_end=True,\n",
    "        packing=True,\n",
    "        completion_only_loss=False,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        resume_from_checkpoint=True\n",
    "    )\n",
    "    \n",
    ")\n",
    "print(\"SFTTrainer 초기화 완료. 학습을 시작합니다.\")\n",
    "\n",
    "# 모델 학습 시작\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9910d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiho1\\anaconda3\\envs\\llmenv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3749' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3749/3750 35:14:22 < 02:49, 0.01 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.621100</td>\n",
       "      <td>0.830179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiho1\\anaconda3\\envs\\llmenv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3749, training_loss=0.12446906923389206, metrics={'train_runtime': 127000.156, 'train_samples_per_second': 0.472, 'train_steps_per_second': 0.03, 'total_flos': 6.937343439195341e+17, 'train_loss': 0.12446906923389206})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0137df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 LoRA 어댑터가 './qwen_squad_finetuned\\squad_lora_adapter'에 저장되었습니다.\n",
      "이제 저장된 어댑터와 기본 모델을 병합하여 새로운 모델을 만들거나, 어댑터를 로드하여 추론에 사용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 학습된 LoRA 어댑터 저장\n",
    "output_adapter_dir = os.path.join(output_dir, \"squad_lora_adapter\")\n",
    "trainer.model.save_pretrained(output_adapter_dir)\n",
    "tokenizer.save_pretrained(output_adapter_dir) # 토크나이저도 함께 저장\n",
    "\n",
    "print(f\"학습된 LoRA 어댑터가 '{output_adapter_dir}'에 저장되었습니다.\")\n",
    "print(\"이제 저장된 어댑터와 기본 모델을 병합하여 새로운 모델을 만들거나, 어댑터를 로드하여 추론에 사용할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3df17ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재까지 학습된 LoRA 어댑터를 './qwen_squad_finetuned\\manual_save_20250527-103754'에 저장합니다...\n",
      "현재 학습 상태가 './qwen_squad_finetuned\\manual_save_20250527-103754'에 성공적으로 저장되었습니다.\n",
      "이 어댑터를 로드하여 추론하거나, 나중에 학습을 재개할 때 사용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 이 코드를 주피터 노트북의 새로운 코드 셀에 넣어 실행하세요.\n",
    "# trainer.train()을 중단한 직후에 실행하면 됩니다.\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 저장할 디렉토리 경로 정의 (현재 시간으로 고유한 이름 생성)\n",
    "# training_arguments.output_dir은 이전에 설정했던 'qwen_squad_finetuned'입니다.\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "manual_save_dir = os.path.join(training_arguments.output_dir, f\"manual_save_{timestamp}\")\n",
    "\n",
    "print(f\"현재까지 학습된 LoRA 어댑터를 '{manual_save_dir}'에 저장합니다...\")\n",
    "\n",
    "# 저장할 디렉토리가 없으면 생성\n",
    "if not os.path.exists(manual_save_dir):\n",
    "    os.makedirs(manual_save_dir)\n",
    "\n",
    "# trainer 객체에서 현재 모델의 LoRA 어댑터 상태를 저장\n",
    "# trainer.model은 PEFT (LoRA) 모델 객체입니다.\n",
    "trainer.model.save_pretrained(manual_save_dir)\n",
    "\n",
    "# 토크나이저도 함께 저장하여 나중에 이 어댑터를 로드할 때 사용할 수 있도록 합니다.\n",
    "tokenizer.save_pretrained(manual_save_dir)\n",
    "\n",
    "print(f\"현재 학습 상태가 '{manual_save_dir}'에 성공적으로 저장되었습니다.\")\n",
    "print(\"이 어댑터를 로드하여 추론하거나, 나중에 학습을 재개할 때 사용할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b6e0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:22<00:00, 27.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어댑터 로드 및 연결 완료.\n",
      "\n",
      "--- 테스트 추론 ---\n",
      "질문: When did Beyonce become famous?\n",
      "맥락: Beyoncé Giselle Knowles-Carter (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\n",
      "답변: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "late 1990s\n",
      "\n",
      "--- LoRA 어댑터 병합 시작 (선택 사항) ---\n",
      "어댑터 병합 완료.\n",
      "병합된 모델이 './qwen_squad_finetuned_merged'에 저장되었습니다.\n",
      "작업 완료.\n"
     ]
    }
   ],
   "source": [
    "# --- 학습된 모델로 추론하기 (선택 사항) ---\n",
    "from peft import PeftModel\n",
    "from transformers import pipeline\n",
    "\n",
    "# 저장된 어댑터 로드\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config=bnb_config # 4bit 양자화 사용 시 주석 해제\n",
    ")\n",
    "\n",
    "# 어댑터를 기본 모델에 연결\n",
    "adapter_path = os.path.join(output_dir, \"squad_lora_adapter\")\n",
    "model_with_adapter = PeftModel.from_pretrained(loaded_model, adapter_path)\n",
    "print(\"어댑터 로드 및 연결 완료.\")\n",
    "\n",
    "# 추론을 위한 파이프라인 생성 (Qwen 토크나이저 사용)\n",
    "# Qwen 모델은 채팅 템플릿 적용이 필요합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "def generate_answer(question_text, context_text, model, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"맥락: {context_text}\\n질문: {question_text}\"}\n",
    "    ]\n",
    "    # 채팅 템플릿 적용\n",
    "    # 이 부분은 Qwen1.5 모델의 최신 채팅 템플릿에 따라 약간 달라질 수 있습니다.\n",
    "    # 일반적으로 apply_chat_template을 사용합니다.\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # 입력 프롬프트 부분을 제외하고 생성된 텍스트만 디코딩\n",
    "    generated_text = tokenizer.decode(generated_ids[0][model_inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "# 예시 질문과 맥락\n",
    "test_context = \"Beyoncé Giselle Knowles-Carter (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\"\n",
    "test_question = \"When did Beyonce become famous?\"\n",
    "\n",
    "print(\"\\n--- 테스트 추론 ---\")\n",
    "print(f\"질문: {test_question}\")\n",
    "print(f\"맥락: {test_context}\")\n",
    "answer = generate_answer(test_question, test_context, model_with_adapter, tokenizer)\n",
    "print(f\"답변: {answer}\")\n",
    "\n",
    "\n",
    "# --- LoRA 어댑터와 기본 모델 병합 및 저장 (선택 사항) ---\n",
    "# 파인튜닝된 모델을 일반적인 형태로 저장하여 쉽게 배포하거나 사용할 수 있도록 합니다.\n",
    "# GPU 메모리가 충분해야 이 작업을 수행할 수 있습니다.\n",
    "# qlora를 사용했다면, merge_and_unload() 전에 prepare_model_for_kbit_training을 해제해야 할 수 있습니다.\n",
    "\n",
    "print(\"\\n--- LoRA 어댑터 병합 시작 (선택 사항) ---\")\n",
    "try:\n",
    "    # QLoRA를 사용했다면 model_with_adapter.merge_and_unload()를 사용하기 전에\n",
    "    # 모델을 CPU로 이동시키거나 (model_with_adapter.to('cpu'))\n",
    "    # 일부 설정을 비활성화해야 할 수 있습니다.\n",
    "    # 일반적인 LoRA에서는 바로 merge_and_unload()를 사용합니다.\n",
    "    merged_model = model_with_adapter.merge_and_unload()\n",
    "    print(\"어댑터 병합 완료.\")\n",
    "\n",
    "    # 병합된 모델 저장\n",
    "    merged_model_output_dir = \"./qwen_squad_finetuned_merged\"\n",
    "    merged_model.save_pretrained(merged_model_output_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(merged_model_output_dir)\n",
    "    print(f\"병합된 모델이 '{merged_model_output_dir}'에 저장되었습니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"모델 병합 중 오류 발생: {e}\")\n",
    "    print(\"GPU 메모리 부족이 원인일 수 있습니다. 병합은 많은 메모리를 요구합니다.\")\n",
    "\n",
    "print(\"작업 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- LoRA 어댑터 병합 시작 (선택 사항) ---\")\n",
    "try:\n",
    "    # QLoRA를 사용했다면 model_with_adapter.merge_and_unload()를 사용하기 전에\n",
    "    # 모델을 CPU로 이동시키거나 (model_with_adapter.to('cpu'))\n",
    "    # 일부 설정을 비활성화해야 할 수 있습니다.\n",
    "    # 일반적인 LoRA에서는 바로 merge_and_unload()를 사용합니다.\n",
    "    merged_model = model_with_adapter.merge_and_unload()\n",
    "    print(\"어댑터 병합 완료.\")\n",
    "\n",
    "    # 병합된 모델 저장\n",
    "    merged_model_output_dir = \"./qwen_squad_finetuned_merged\"\n",
    "    merged_model.save_pretrained(merged_model_output_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(merged_model_output_dir)\n",
    "    print(f\"병합된 모델이 '{merged_model_output_dir}'에 저장되었습니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"모델 병합 중 오류 발생: {e}\")\n",
    "    print(\"GPU 메모리 부족이 원인일 수 있습니다. 병합은 많은 메모리를 요구합니다.\")\n",
    "\n",
    "print(\"작업 완료.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extra_llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
